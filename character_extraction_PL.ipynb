{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing Features from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/eileen/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/tmp/ipykernel_5413/2182223124.py:95: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  wn_input = corpus['head_of_head'].apply(lambda word: pd.Series(wn.synsets(word)))\n",
      "/tmp/ipykernel_5413/2182223124.py:102: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  test = wn_input[0].apply(lambda syn: pd.Series(syn.lowest_common_hypernyms(per)))\n",
      "Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
      "/tmp/ipykernel_5413/2182223124.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corpus['CN'][i]=val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import pickle\n",
    "import yake\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('popular')\n",
    "\n",
    "from lib import data_utils, preprocess\n",
    "\n",
    "#get path\n",
    "src_path = os.getcwd()\n",
    "\n",
    "datapath = src_path + '/Data/'\n",
    "PLcopath = datapath + 'AnnotatedPLData/PLCoref'\n",
    "PLpath = datapath + 'AnnotatedPLData/PLTexts'\n",
    "CENcopath = datapath + 'AnnotatedCENData/CENCoref'\n",
    "CENpath = datapath + 'AnnotatedCENData/CENTexts'\n",
    "ONpath = datapath + 'AnnotatedONData/ONCoref'\n",
    "\n",
    "PLInter = src_path + '/output/IntermediateFilesPL/'\n",
    "CENInter = src_path + '/output/IntermediateFilesCEN/'\n",
    "ONInter = src_path + '/output/IntermediateFilesON/'\n",
    "\n",
    "#import data\n",
    "\n",
    "#create empty data frame\n",
    "PL_data = pd.DataFrame(columns=['corpusID', 'character', 'animacy', 'coref_chain', 'chain_head', 'head_of_head', 'chain_len', 'CL'])\n",
    "\n",
    "#append all PL texts and features into one dataframe\n",
    "for n in range(1,47):\n",
    "# for n in range(1,2):\n",
    "\n",
    "    print(n)\n",
    "\n",
    "    #get story path\n",
    "    storycopath = PLcopath +'/story' + str(n) + '.txt'\n",
    "    storypath = PLpath + '/story' + str(n) + '.txt'\n",
    "    storyid = 'story'+ str(n)\n",
    "\n",
    "    #read in story\n",
    "    corpus = data_utils.read_story(storycopath)\n",
    "\n",
    "    # read in from intermediate files\n",
    "    # list of features\n",
    "    # features = [\"CN\", \"Dep\", \"NER\", \"SS\", \"Triple\", \"WN\"]\n",
    "\n",
    "    # for f in features:\n",
    "    #     #empty list\n",
    "    #     feat = []\n",
    "    #     with open(PLInter + f + 'FeatureBoolean'+'/Story' + str(n) + '.txt', 'r') as doc:\n",
    "    #         for line in doc:\n",
    "    #             feat.append(eval(line.rstrip()))\n",
    "    \n",
    "    #     corpus[f] = feat\n",
    "\n",
    "    #get ss feature\n",
    "    sslist = preprocess.semantic_subj(storypath)\n",
    "    #remove leading The/A's in the sematic list\n",
    "    sslist = [re.sub('^(The |A )','',s, flags=re.IGNORECASE) for s in sslist]\n",
    "    pattern = '|'.join(sslist)\n",
    "    pattern = pattern.replace('?|','')\n",
    "    pattern = pattern.replace('!|','')\n",
    "    pattern = pattern.replace('.|','')\n",
    "    pattern = pattern.replace('(|','')\n",
    "    pattern = pattern.replace('(','')\n",
    "    pattern = pattern.replace(')','')\n",
    "    \n",
    "    #create binary flag variable for ss feat\n",
    "    corpus['SS'] = corpus['head_of_head'].str.contains(pattern)\n",
    "    corpus['SS'] = corpus['SS'].replace({True:1, False:0})\n",
    "\n",
    "    #get ner feature\n",
    "    nerlist = preprocess.ner_person(storypath)\n",
    "    pattern = '|'.join(nerlist)\n",
    "    pattern = pattern.replace('(','')\n",
    "    pattern = pattern.replace(')','')\n",
    "    pattern = pattern.replace('[','')\n",
    "    pattern = pattern.replace(']','')\n",
    "\n",
    "    #create binary flag variable for ss feat\n",
    "    corpus['NER'] = corpus['head_of_head'].str.contains(pattern)\n",
    "    corpus['NER'] = corpus['NER'].replace({True:1, False:0})\n",
    "\n",
    "\n",
    "    #create binary flag variable for wn feat\n",
    "    #get wordnet synset of head of chain\n",
    "    wn_input = corpus['head_of_head'].apply(lambda word: pd.Series(wn.synsets(word)))\n",
    "   \n",
    "    #fill blanks with unrelated word to person\n",
    "    wn_input[0] = wn_input[0].fillna(wn.synset('strong.a.01'))\n",
    "   \n",
    "    # get common synonym with person\n",
    "    per = wn.synset('person.n.01')\n",
    "    test = wn_input[0].apply(lambda syn: pd.Series(syn.lowest_common_hypernyms(per)))\n",
    "   \n",
    "    # test if head of chain related to person\n",
    "    corpus['WN']= test[0]==per\n",
    "    corpus['WN'] = corpus['WN'].replace({True:1, False:0})\n",
    "\n",
    "    #get dp feat\n",
    "    dplist = preprocess.dep_link(storypath)\n",
    "    dplist = list(set(dplist))\n",
    "    pattern='|'.join(dplist)\n",
    "    #remove punct\n",
    "    pattern = pattern.replace('?|','')\n",
    "    pattern = pattern.replace('!|','')\n",
    "    pattern = pattern.replace('.|','')\n",
    "    pattern = pattern.replace('(|','')\n",
    "    pattern = pattern.replace(')|','')\n",
    "    pattern = pattern.replace('(','')\n",
    "    pattern = pattern.replace(')','')\n",
    "\n",
    "    #create binary flag variable for dp feat\n",
    "    corpus['DP'] = corpus['head_of_head'].str.contains(pattern)\n",
    "    corpus['DP'] = corpus['DP'].replace({True:1, False:0})\n",
    "\n",
    "    #get triple feat\n",
    "    tplist = preprocess.triple(storypath)\n",
    "    tplist = list(set(tplist))\n",
    "    #remove leading The/A's in the sematic list\n",
    "    tplist = [re.sub('^(The |A )','',s, flags=re.IGNORECASE) for s in tplist]\n",
    "    pattern='|'.join(tplist)\n",
    "    pattern = pattern.replace('?|','')\n",
    "    pattern = pattern.replace('!|','')\n",
    "    pattern = pattern.replace('.|','')\n",
    "    pattern = pattern.replace('(|','')\n",
    "    pattern = pattern.replace(')|','')\n",
    "    pattern = pattern.replace('(','')\n",
    "    pattern = pattern.replace(')','')\n",
    "\n",
    "    #create binary flag variable for ss feat\n",
    "    corpus['TP'] = corpus['head_of_head'].str.contains(pattern)\n",
    "    corpus['TP'] = corpus['TP'].replace({True:1, False:0})\n",
    "\n",
    "    # get conceptnet feat\n",
    "    urlreq = 'https://api.conceptnet.io/c/en/'+corpus['head_of_head']\n",
    "\n",
    "    #default no presence of person mentioned\n",
    "    corpus['CN'] = 0\n",
    "\n",
    "    for i in range(len(urlreq)):\n",
    "\n",
    "        #make request to concept net api\n",
    "        response = requests.get(urlreq[i])\n",
    "        obj = response.json()\n",
    "        #get list of edges\n",
    "        cnlist = [edge['@id'] for edge in obj['edges']]\n",
    "\n",
    "        #if person is in list then flag\n",
    "        if any('person' in s for s in cnlist):\n",
    "            val = 1\n",
    "            corpus['CN'][i]=val\n",
    "    \n",
    "    #create feature for freq of head of chain term in text\n",
    "    tf_dict = preprocess.term_freq(storypath)\n",
    "    corpus['TF'] = corpus['head_of_head'].map(tf_dict)\n",
    "\n",
    "    #create feature that contains keyword extraction score from yake\n",
    "    f = open(storypath, 'r', encoding='ISO-8859-1')\n",
    "    text = f.read()\n",
    "    f.close()  \n",
    "\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    keywords = dict(kw_extractor.extract_keywords(text))\n",
    "    corpus['YK_SC'] = corpus['head_of_head'].map(keywords)\n",
    "    corpus['YK_SC'] = corpus['YK_SC'].fillna(100)\n",
    "\n",
    "    #append to dataframe\n",
    "    PL_data = pd.concat([PL_data, corpus], ignore_index=True)\n",
    "\n",
    "PL_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Features from Intermediate File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpusID</th>\n",
       "      <th>character</th>\n",
       "      <th>animacy</th>\n",
       "      <th>coref_chain</th>\n",
       "      <th>chain_head</th>\n",
       "      <th>head_of_head</th>\n",
       "      <th>chain_len</th>\n",
       "      <th>CL</th>\n",
       "      <th>CN</th>\n",
       "      <th>Dep</th>\n",
       "      <th>NER</th>\n",
       "      <th>SS</th>\n",
       "      <th>Triple</th>\n",
       "      <th>WN</th>\n",
       "      <th>TF</th>\n",
       "      <th>YK_SC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>story1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[ dragon ,  he ,  he ,  the dragon ,  He ,  hi...</td>\n",
       "      <td>dragon</td>\n",
       "      <td>dragon</td>\n",
       "      <td>43</td>\n",
       "      <td>2.499540</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.015363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>story1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[ princess ,  the tsar 's daughter ,  her ,  h...</td>\n",
       "      <td>princess</td>\n",
       "      <td>princess</td>\n",
       "      <td>23</td>\n",
       "      <td>0.990763</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.063038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>story1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[ tsar ,  tsar ,  father ,  tsar ,  her father...</td>\n",
       "      <td>tsar</td>\n",
       "      <td>tsar</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.065380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.088675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>story1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[ princess' dog ,  a little dog that had follo...</td>\n",
       "      <td>princess' dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.442574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007059</td>\n",
       "      <td>0.173527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>story1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[ tsarina ,  mother ,  tsarina ,  tsarina ]</td>\n",
       "      <td>tsarina</td>\n",
       "      <td>tsarina</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.442574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>story46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[ the entire enemy army ,  the enemy army ,  h...</td>\n",
       "      <td>the entire enemy army</td>\n",
       "      <td>army</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.308434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007293</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>story46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[ this ,  the best solution ]</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.604613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>story46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[ a horse ,  a horse ]</td>\n",
       "      <td>a horse</td>\n",
       "      <td>horse</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.604613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>story46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[ an even better horse ,  his horse ,  his hor...</td>\n",
       "      <td>an even better horse</td>\n",
       "      <td>horse</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.486142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>story46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[ the hand ,  the hand ]</td>\n",
       "      <td>the hand</td>\n",
       "      <td>hand</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.604613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1912 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     corpusID character animacy  \\\n",
       "0      story1         1       1   \n",
       "1      story1         1       1   \n",
       "2      story1         1       1   \n",
       "3      story1         0       1   \n",
       "4      story1         1       1   \n",
       "...       ...       ...     ...   \n",
       "1907  story46         0       1   \n",
       "1908  story46         0       1   \n",
       "1909  story46         0       1   \n",
       "1910  story46         0       1   \n",
       "1911  story46         0       1   \n",
       "\n",
       "                                            coref_chain  \\\n",
       "0     [ dragon ,  he ,  he ,  the dragon ,  He ,  hi...   \n",
       "1     [ princess ,  the tsar 's daughter ,  her ,  h...   \n",
       "2     [ tsar ,  tsar ,  father ,  tsar ,  her father...   \n",
       "3     [ princess' dog ,  a little dog that had follo...   \n",
       "4           [ tsarina ,  mother ,  tsarina ,  tsarina ]   \n",
       "...                                                 ...   \n",
       "1907  [ the entire enemy army ,  the enemy army ,  h...   \n",
       "1908                      [ this ,  the best solution ]   \n",
       "1909                             [ a horse ,  a horse ]   \n",
       "1910  [ an even better horse ,  his horse ,  his hor...   \n",
       "1911                           [ the hand ,  the hand ]   \n",
       "\n",
       "                   chain_head head_of_head chain_len        CL   CN  Dep  NER  \\\n",
       "0                     dragon        dragon        43  2.499540  1.0  1.0  0.0   \n",
       "1                   princess      princess        23  0.990763  1.0  1.0  0.0   \n",
       "2                       tsar          tsar         9 -0.065380  1.0  1.0  0.0   \n",
       "3              princess' dog           dog         4 -0.442574  1.0  1.0  0.0   \n",
       "4                    tsarina       tsarina         4 -0.442574  1.0  1.0  0.0   \n",
       "...                       ...          ...       ...       ...  ...  ...  ...   \n",
       "1907   the entire enemy army          army         7 -0.308434  0.0  1.0  0.0   \n",
       "1908                    this          this         2 -0.604613  0.0  0.0  0.0   \n",
       "1909                 a horse         horse         2 -0.604613  0.0  0.0  0.0   \n",
       "1910    an even better horse         horse         4 -0.486142  0.0  0.0  0.0   \n",
       "1911                the hand          hand         2 -0.604613  0.0  0.0  0.0   \n",
       "\n",
       "       SS  Triple   WN        TF       YK_SC  \n",
       "0     1.0     1.0  0.0  0.040000    0.015363  \n",
       "1     1.0     0.0  1.0  0.014118    0.063038  \n",
       "2     1.0     1.0  1.0  0.011765    0.088675  \n",
       "3     1.0     1.0  0.0  0.007059    0.173527  \n",
       "4     1.0     0.0  1.0  0.004706  100.000000  \n",
       "...   ...     ...  ...       ...         ...  \n",
       "1907  0.0     0.0  0.0  0.007293    0.056700  \n",
       "1908  1.0     0.0  0.0  0.000000  100.000000  \n",
       "1909  0.0     1.0  0.0  0.004862  100.000000  \n",
       "1910  0.0     1.0  0.0  0.004862  100.000000  \n",
       "1911  0.0     0.0  0.0  0.002431  100.000000  \n",
       "\n",
       "[1912 rows x 16 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from lib import data_utils, preprocess\n",
    "\n",
    "#get path\n",
    "src_path = os.getcwd()\n",
    "\n",
    "datapath = src_path + '/Data/'\n",
    "PLcopath = datapath + 'AnnotatedPLData/PLCoref'\n",
    "PLpath = datapath + 'AnnotatedPLData/PLTexts'\n",
    "CENcopath = datapath + 'AnnotatedCENData/CENCoref'\n",
    "CENpath = datapath + 'AnnotatedCENData/CENCoref'\n",
    "ONpath = datapath + 'AnnotatedONData/ONCoref'\n",
    "\n",
    "PLInter = src_path + '/output/IntermediateFilesPL/'\n",
    "CENInter = src_path + '/output/IntermediateFilesCEN/'\n",
    "ONInter = src_path + '/output/IntermediateFilesON/'\n",
    "\n",
    "#import data\n",
    "\n",
    "#create empty data frame\n",
    "PL_Int_data = pd.DataFrame(columns=['corpusID', 'character', 'animacy', 'coref_chain', 'chain_head', 'head_of_head', 'chain_len', 'CL'])\n",
    "\n",
    "#append all PL texts and features into one dataframe\n",
    "for n in range(1,47):\n",
    "# for n in range(1,2):\n",
    "\n",
    "    print(n)\n",
    "\n",
    "    #get story path\n",
    "    storycopath = PLcopath +'/story' + str(n) + '.txt'\n",
    "    storypath = PLpath + '/story' + str(n) + '.txt'\n",
    "    storyid = 'story'+ str(n)\n",
    "\n",
    "    #read in story\n",
    "    corpus = data_utils.read_story(storycopath)\n",
    "\n",
    "    #read in from intermediate files\n",
    "    #list of features\n",
    "    features = [\"CN\", \"Dep\", \"NER\", \"SS\", \"Triple\", \"WN\"]\n",
    "\n",
    "    for f in features:\n",
    "        #empty list\n",
    "        feat = []\n",
    "        with open(PLInter + f + 'FeatureBoolean'+'/Story' + str(n) + '.txt', 'r') as doc:\n",
    "            for line in doc:\n",
    "                feat.append(eval(line.rstrip()))\n",
    "    \n",
    "        corpus[f] = feat\n",
    "    \n",
    "    #creat feature for term freq\n",
    "    tf_dict = preprocess.term_freq(storypath)\n",
    "    corpus['TF'] = corpus['head_of_head'].map(tf_dict)\n",
    "\n",
    "    #create feature that contains keyword extraction score from yake\n",
    "    f = open(storypath, 'r', encoding='ISO-8859-1')\n",
    "    text = f.read()\n",
    "    f.close()  \n",
    "\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    keywords = dict(kw_extractor.extract_keywords(text))\n",
    "    corpus['YK_SC'] = corpus['head_of_head'].map(keywords)\n",
    "    corpus['YK_SC'] = corpus['YK_SC'].fillna(100)\n",
    "\n",
    "    #append to dataframe\n",
    "    PL_Int_data = pd.concat([PL_Int_data, corpus], ignore_index=True)\n",
    "PL_Int_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90 accuracy with a standard deviation of 0.03\n",
      "0.81 f1 score with a standard deviation of 0.06\n"
     ]
    }
   ],
   "source": [
    "# simple model with their features\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "data_y = PL_Int_data[\"character\"].astype('int')\n",
    "data_x = PL_Int_data[[\"CL\", \"CN\", \"Dep\", \"NER\", \"SS\", \"Triple\", \"WN\", \"animacy\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, random_state=1)\n",
    "\n",
    "rbf_svc = svm.SVC(kernel = 'rbf', C=.5, gamma=1)\n",
    "\n",
    "#fit\n",
    "rbf_svc.fit(X_train, y_train)\n",
    "\n",
    "#save\n",
    "dump(rbf_svc, \"models/PLjahan_model.joblib\")\n",
    "rbf_svc = load('models/PLjahan_model.joblib') \n",
    "\n",
    "cv_results = cross_validate(rbf_svc, X_train, y_train, cv = 10, scoring=('f1', 'accuracy'), return_train_score=True)\n",
    "\n",
    "# print(cv_results['train_accuracy'])\n",
    "# print(cv_results['test_accuracy'])\n",
    "# print(cv_results['train_f1'])\n",
    "# print(cv_results['test_f1'])\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (cv_results['test_accuracy'].mean(), cv_results['test_accuracy'].std()))\n",
    "print(\"%0.2f f1 score with a standard deviation of %0.2f\" % (cv_results['test_f1'].mean(), cv_results['test_f1'].std()))\n",
    "# cv = StratifiedShuffleSplit(n_splits=10, test_size=.2, random_state=42)\n",
    "\n",
    "# C_range = np.logspace(-2,10,13)\n",
    "# gamma_range = np.logspace(-9,3,13)\n",
    "\n",
    "# grid = GridSearchCV(svm.SVC(kernel='rbf'), param_grid=dict(gamma = gamma_range, C= C_range), cv=cv)\n",
    "\n",
    "# grid.fit(X_train,y_train)\n",
    "\n",
    "# print(grid.best_params_, grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90 accuracy with a standard deviation of 0.03\n",
      "0.80 f1 score for character class with a standard deviation of 0.06\n"
     ]
    }
   ],
   "source": [
    "# simple model with their features + term freq + yake score\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "data_y = PL_Int_data[\"character\"].astype('int')\n",
    "data_x = PL_Int_data[[\"CL\", \"CN\", \"Dep\", \"NER\", \"SS\", \"Triple\", \"WN\", \"YK_SC\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, random_state=1)\n",
    "\n",
    "rbf_svc_tf = svm.SVC(kernel = 'rbf', C=.5, gamma=1)\n",
    "\n",
    "#fit\n",
    "rbf_svc_tf.fit(X_train, y_train)\n",
    "\n",
    "#save\n",
    "dump(rbf_svc_tf, \"models/PLjahan_model_tf.joblib\")\n",
    "rbf_svc_tf = load('models/PLjahan_model_tf.joblib') \n",
    "\n",
    "cv_results = cross_validate(rbf_svc_tf, X_train, y_train, cv = 10, scoring=('f1', 'accuracy', 'f1_macro'), return_train_score=True)\n",
    "\n",
    "# print(cv_results['train_accuracy'])\n",
    "# print(cv_results['test_accuracy'])\n",
    "# print(cv_results['train_f1'])\n",
    "# print(cv_results['test_f1'])\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (cv_results['test_accuracy'].mean(), cv_results['test_accuracy'].std()))\n",
    "print(\"%0.2f f1 score for character class with a standard deviation of %0.2f\" % (cv_results['test_f1'].mean(), cv_results['test_f1'].std()))\n",
    "# print(\"%0.2f f1 score with a standard deviation of %0.2f\" % (cv_results['test_f1_macro'].mean(), cv_results['test_f1'].std()))\n",
    "# print(\"%0.2f f1 score for non character class\" % (2*cv_results['test_f1_macro'].mean() - cv_results['test_f1'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['YK_SC'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb#ch0000007?line=13'>14</a>\u001b[0m PL_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mData/PL.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb#ch0000007?line=15'>16</a>\u001b[0m data_y \u001b[39m=\u001b[39m PL_data[\u001b[39m\"\u001b[39m\u001b[39mcharacter\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb#ch0000007?line=16'>17</a>\u001b[0m data_x \u001b[39m=\u001b[39m PL_data[[\u001b[39m\"\u001b[39;49m\u001b[39mCL\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mCN\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mDP\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mNER\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mSS\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mTP\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mWN\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39manimacy\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mYK_SC\u001b[39;49m\u001b[39m\"\u001b[39;49m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb#ch0000007?line=18'>19</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(data_x, data_y, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb#ch0000007?line=20'>21</a>\u001b[0m rbf_svc_1 \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39mSVC(kernel \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m'\u001b[39m, C\u001b[39m=\u001b[39m\u001b[39m.5\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/char_ext_2/lib/python3.8/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/char_ext_2/lib/python3.8/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/char_ext_2/lib/python3.8/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['YK_SC'] not in index\""
     ]
    }
   ],
   "source": [
    "# simple model \n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "PL_data = pd.read_csv('Data/PL.csv')\n",
    "\n",
    "data_y = PL_data[\"character\"].astype('int')\n",
    "data_x = PL_data[[\"CL\", \"CN\", \"DP\", \"NER\", \"SS\", \"TP\", \"WN\", \"animacy\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, random_state=1)\n",
    "\n",
    "rbf_svc_1 = svm.SVC(kernel = 'rbf', C=.5, gamma=1)\n",
    "\n",
    "#fit\n",
    "rbf_svc_1.fit(X_train, y_train)\n",
    "\n",
    "#save\n",
    "dump(rbf_svc_1, \"models/PLallen_model.joblib\")\n",
    "rbf_svc_1 = load('models/PLallen_model.joblib') \n",
    "\n",
    "cv_results = cross_validate(rbf_svc_1, X_train, y_train, cv = 10, scoring=('f1', 'accuracy', 'f1_macro'), return_train_score=True)\n",
    "\n",
    "# print(cv_results['train_accuracy'])\n",
    "# print(cv_results['test_accuracy'])\n",
    "# print(cv_results['train_f1'])\n",
    "# print(cv_results['test_f1'])\n",
    "# print(cv_results['test_f1_macro'])\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (cv_results['test_accuracy'].mean(), cv_results['test_accuracy'].std()))\n",
    "print(\"%0.2f f1 score for character class with a standard deviation of %0.2f\" % (cv_results['test_f1'].mean(), cv_results['test_f1'].std()))\n",
    "# print(\"%0.2f f1 score with a standard deviation of %0.2f\" % (cv_results['test_f1_macro'].mean(), cv_results['test_f1'].std()))\n",
    "# print(\"%0.2f f1 score fro non character class\" % (2*cv_results['test_f1_macro'].mean() - cv_results['test_f1'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['CN', 'DP', 'NER', 'SS', 'TP', 'WN', 'TF'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb#ch0000008?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb#ch0000008?line=11'>12</a>\u001b[0m data_y \u001b[39m=\u001b[39m PL_data[\u001b[39m\"\u001b[39m\u001b[39mcharacter\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb#ch0000008?line=12'>13</a>\u001b[0m data_x \u001b[39m=\u001b[39m PL_data[[\u001b[39m\"\u001b[39;49m\u001b[39mCL\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mCN\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mDP\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mNER\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mSS\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mTP\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mWN\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mTF\u001b[39;49m\u001b[39m\"\u001b[39;49m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb#ch0000008?line=14'>15</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(data_x, data_y, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/projects/DeepZen/Char_Ext/character_extraction_PL.ipynb#ch0000008?line=16'>17</a>\u001b[0m rbf_svc_1tf \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39mSVC(kernel \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m'\u001b[39m, C\u001b[39m=\u001b[39m\u001b[39m.5\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/char_ext_2/lib/python3.8/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/char_ext_2/lib/python3.8/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/char_ext_2/lib/python3.8/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['CN', 'DP', 'NER', 'SS', 'TP', 'WN', 'TF'] not in index\""
     ]
    }
   ],
   "source": [
    "# simple model \n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data_y = PL_data[\"character\"].astype('int')\n",
    "data_x = PL_data[[\"CL\", \"CN\", \"DP\", \"NER\", \"SS\", \"TP\", \"WN\", \"TF\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, random_state=1)\n",
    "\n",
    "rbf_svc_1tf = svm.SVC(kernel = 'rbf', C=.5, gamma=1)\n",
    "\n",
    "#fit\n",
    "rbf_svc_1tf.fit(X_train, y_train)\n",
    "\n",
    "#save\n",
    "dump(rbf_svc_1tf, \"models/PLallen_model_tf.joblib\")\n",
    "rbf_svc_1tf = load('models/PLallen_model_tf.joblib') \n",
    "\n",
    "cv_results = cross_validate(rbf_svc_1tf, X_train, y_train, cv = 10, scoring=('f1', 'accuracy'), return_train_score=True)\n",
    "\n",
    "# print(cv_results['train_accuracy'])\n",
    "# print(cv_results['test_accuracy'])\n",
    "# print(cv_results['train_f1'])\n",
    "# print(cv_results['test_f1'])\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (cv_results['test_accuracy'].mean(), cv_results['test_accuracy'].std()))\n",
    "print(\"%0.2f f1 score with a standard deviation of %0.2f\" % (cv_results['test_f1'].mean(), cv_results['test_f1'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PL_data.to_csv(\"Data/PL.csv\")\n",
    "PL_Int_data.to_csv(\"Data/PLInter.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('char_ext_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ab110a997218d36eb782d7088a10d031e5f038772d20aa5829c90f0ae252251"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
